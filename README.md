# KNN
K Nearest Neighbours:
The k-Nearest Neighbors (KNN) algorithm is a fundamental and intuitive machine learning technique for classification and regression tasks. Rooted in the concept of similarity, KNN operates by assigning a label or value to an unlabeled data point based on the majority class or the average of its k nearest neighbours in the feature space. In essence, KNN relies on the assumption that similar instances tend to share similar outcomes. This non-parametric algorithm requires no explicit model training. It can adapt to various data distributions, making it particularly useful for simple classification tasks and cases where underlying patterns are not well-defined. However, while KNN offers simplicity and interpretability, it can be sensitive to noise and high-dimensional data, necessitating careful parameter selection and preprocessing.

Pictionary Dataset: The Dataset to be used can be found in data.npy file. This is a npy(numpy) file of dataset where each row corresponds to a datapoint and the number of rows is equivalent to the number of examples in the dataset. The first column in each row of the dataset is the game id, the second column is the embeddings generated by ResNet and third column is the embeddings generated by VIT. The fourth column is the label name(ground truth) and the final column is the guess time which you can ignore. You can read more about Resnets and VITs


Exploratory Data Analysis: Exploratory dataset visualization and analysis form the cornerstone of data exploration in various domains, serving as a critical initial step before diving into more complex data modeling and decision-making processes. This practice involves visually inspecting and comprehending the distribution, relationships, and patterns within a dataset. Its importance lies in its ability to unearth crucial insights, anomalies, and trends that might not be apparent through raw data alone. Through exploratory visualization, data practitioners can quickly identify outliers, clusters, and potential errors, leading to data cleaning and preprocessing. Furthermore, it aids in feature selection by revealing the impact of different variables on the target outcome. This process allows for the identification of potential correlations and dependencies, which can significantly impact the choice of algorithms and models during subsequent analysis. In addition, exploratory analysis empowers domain experts to formulate hypotheses and refine research questions, ultimately guiding the direction of the entire analysis. It provides the necessary context for framing questions that would otherwise be overlooked in the absence of visual cues and statistical insights

Task-1: Draw a graph that shows the distribution of the various labels across the entire dataset. You are allowed to use standard libraries like Matplotlib.

KNN Implementation: You must implement the KNN algorithm from scratch using Python classes in this section. This approach encapsulates the KNN model and its methods within a single class, enhancing code readability, reusability, and maintainability. You can easily manage hyperparameters, data preprocessing, and result interpretation by designing the class. Additionally, you can extend the class to incorporate specific functionalities like cross-validation, hyperparameter tuning, or custom distance metrics, aligning the KNN algorithm with your goals.

Task-2: The KNN algorithm relies on hyperparameters that significantly impact its performance and behavior. One of the crucial hyperparameters is ’k’, representing the number of nearest neighbors considered for classification or regression. A smaller ’k’ value might lead to a more flexible model, capturing local patterns, while a larger ’k’ value could result in a smoother decision boundary, capturing broader trends. Additionally, the choice of distance metric, such as Euclidean, Manhattan, or cosine distance, significantly influences how the algorithm measures similarity between data points. Other hyperparameters may involve weighting schemes, where closer neighbors have higher influence, and 3 data preprocessing techniques, such as feature scaling, that can affect the algorithm’s sensitivity to input attributes. The thoughtful selection of these hyperparameters plays a vital role in achieving optimal KNN model performance for specific tasks. Create a KNN class where you implement the following:
You should not use sklearn for this.
1. Create a class where you can modify and access the encoder type, k, and distance metric (and any required parameter)of the class 
2. Return the inference (prediction) when given the above parameters (encoder type, k, and distance metric). 
3. Return the validation f-1 score, accuracy, precision, and recall after splitting the provided dataset into train and val subsets. You are allowed to use sklearn metrics for this part.

Hyperparameter Tuning:
1. Find the best (k, encoder, distance metric) triplet that gives the best validation accuracy for a given data split (your choice). 
2. Print an Ordered rank list of top 20 such triplets. 
3. Plot k vs accuracy given a choice(yours) of any given distance, encoder pair (with a constant data split).

Testing:
1. Create a bash script that when given data from a file in the form of a list of test embeddings and test labels as a npy file exactly in the same format as the dataset file given to you, return the same metrics as discussed in task-1
Note: 1. The bash script should take the path to the file as an input. 
2. It should print out in a table the accuracy, f1-score, recall, and precision.
3. Proper error handling should be present 
4. There will be a penalty if this script does not work as expected.

Optimization:
1. Is it possible to improve the execution time of the program? Hint: Use Vectorization. 
2. Plot inference time for initial KNN model, best KNN model, most optimized KNN model, and the default sklearn KNN model. 
3. plot the inference time vs train dataset size for initial KNN model, best KNN model, most optimized KNN model, and the default sklearn KNN model.

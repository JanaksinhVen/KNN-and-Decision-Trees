K-Nearest Neighbours:

The k-Nearest Neighbors (KNN) algorithm is a fundamental and intuitive machine learning technique for classification and regression tasks. Rooted in the concept of similarity, KNN operates by assigning a label or value to an unlabeled data point based on the majority class or the average of its k nearest neighbours in the feature space. In essence, KNN relies on the assumption that similar instances tend to share similar outcomes. This non-parametric algorithm requires no explicit model training. It can adapt to various data distributions, making it particularly useful for simple classification tasks and cases where underlying patterns are not well-defined. However, while KNN offers simplicity and interpretability, it can be sensitive to noise and high-dimensional data, necessitating careful parameter selection and preprocessing.

Pictionary Dataset: The Dataset to be used can be found in data.npy file. This is a npy(numpy) file of dataset where each row corresponds to a datapoint and the number of rows is equivalent to the number of examples in the dataset. The first column in each row of the dataset is the game id, the second column is the embeddings generated by ResNet and third column is the embeddings generated by VIT. The fourth column is the label name(ground truth) and the final column is the guess time which you can ignore. You can read more about Resnets and VITs


Exploratory Data Analysis: Exploratory dataset visualization and analysis form the cornerstone of data exploration in various domains, serving as a critical initial step before diving into more complex data modeling and decision-making processes. This practice involves visually inspecting and comprehending the distribution, relationships, and patterns within a dataset. Its importance lies in its ability to unearth crucial insights, anomalies, and trends that might not be apparent through raw data alone. Through exploratory visualization, data practitioners can quickly identify outliers, clusters, and potential errors, leading to data cleaning and preprocessing. Furthermore, it aids in feature selection by revealing the impact of different variables on the target outcome. This process allows for the identification of potential correlations and dependencies, which can significantly impact the choice of algorithms and models during subsequent analysis. In addition, exploratory analysis empowers domain experts to formulate hypotheses and refine research questions, ultimately guiding the direction of the entire analysis. It provides the necessary context for framing questions that would otherwise be overlooked in the absence of visual cues and statistical insights

[Task-1]: Draw a graph that shows the distribution of the various labels across the entire dataset. You are allowed to use standard libraries like Matplotlib.

KNN Implementation: You must implement the KNN algorithm from scratch using Python classes in this section. This approach encapsulates the KNN model and its methods within a single class, enhancing code readability, reusability, and maintainability. You can easily manage hyperparameters, data preprocessing, and result interpretation by designing the class. Additionally, you can extend the class to incorporate specific functionalities like cross-validation, hyperparameter tuning, or custom distance metrics, aligning the KNN algorithm with your goals.

[Task-2]: The KNN algorithm relies on hyperparameters that significantly impact its performance and behavior. One of the crucial hyperparameters is ’k’, representing the number of nearest neighbors considered for classification or regression. A smaller ’k’ value might lead to a more flexible model, capturing local patterns, while a larger ’k’ value could result in a smoother decision boundary, capturing broader trends. Additionally, the choice of distance metric, such as Euclidean, Manhattan, or cosine distance, significantly influences how the algorithm measures similarity between data points. Other hyperparameters may involve weighting schemes, where closer neighbors have higher influence, and 3 data preprocessing techniques, such as feature scaling, that can affect the algorithm’s sensitivity to input attributes. The thoughtful selection of these hyperparameters plays a vital role in achieving optimal KNN model performance for specific tasks. Create a KNN class where you implement the following:
You should not use sklearn for this.
1. Create a class where you can modify and access the encoder type, k, and distance metric (and any required parameter)of the class 
2. Return the inference (prediction) when given the above parameters (encoder type, k, and distance metric). 
3. Return the validation f-1 score, accuracy, precision, and recall after splitting the provided dataset into train and val subsets. You are allowed to use sklearn metrics for this part.

[Task-3]Hyperparameter Tuning:
1. Find the best (k, encoder, distance metric) triplet that gives the best validation accuracy for a given data split (your choice). 
2. Print an Ordered rank list of top 20 such triplets. 
3. Plot k vs accuracy given a choice(yours) of any given distance, encoder pair (with a constant data split).

[Task-4]Testing:
1. Create a bash script that when given data from a file in the form of a list of test embeddings and test labels as a npy file exactly in the same format as the dataset file given to you, return the same metrics as discussed in task-1
Note: 1. The bash script should take the path to the file as an input. 
2. It should print out in a table the accuracy, f1-score, recall, and precision.
3. Proper error handling should be present 
4. There will be a penalty if this script does not work as expected.

[Task-5]Optimization:
1. Is it possible to improve the execution time of the program? Hint: Use Vectorization. 
2. Plot inference time for initial KNN model, best KNN model, most optimized KNN model, and the default sklearn KNN model. 
3. plot the inference time vs train dataset size for initial KNN model, best KNN model, most optimized KNN model, and the default sklearn KNN model.


Decision Trees:

A Decision Tree is a versatile and interpretable machine learning algorithm used
for both classification and regression tasks. It works by recursively partitioning
the input space into subsets, guided by a series of binary decisions based on
input features. At each node of the tree, the algorithm selects the feature
that best separates the data based on certain criteria, such as Gini impurity or
information gain, and continues splitting until a stopping condition is met, such
as a maximum tree depth or a minimum number of samples in a node. The
result is a tree-like structure where each leaf node corresponds to a predicted
class label (for classification) or a predicted value (for regression).

 MultiLabel Classification:
 
Multilabel classification is a scenario where an instance can be assigned multiple
labels simultaneously. In some cases, a multilabel classification problem might
involve predicting multiple labels for each instance.
The Powerset formulation is a strategy used in multilabel classification where
each unique combination of labels is treated as a separate class. For example,
if you have three labels A, B, and C, you would have 23 = 8 possible label
combinations: {}, {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, and{A, B, C}. Each
instance is then associated with one of these label combinations.
Let C be the number of classes . The multioutput formulation tries to predict
a binary vector b ∈ {0, 1}
C for each input where bi = 1 implies the labeli belongs
to the groundtruth for the particular input. Please note that there can be
multiple i such that bi = 1

[Tasks-1]
1. Build a Decision Tree Classifier Class with the Powerset Formulation
which can be initialized by the following set of hyperparameters = [”max
depth”, ”max features”, ”criterion”]
2. Build a Decision Tree Classifier Class with the MultiOutput Formulation which can be initialized by the following set of hyperparameters =
[”max depth”, ”max features”, ”criterion”]
For this you are allowed to use the inbuilt sklearn decision tree. Note you are
expected to follow the standard datascience practices where you sequentially do
1. data visualization and exploration
2. data preprocessing
3. data featurization
4. train val test splitting
3.4 Hyperparamter Tuning
Given the possible set of hyperparameters given by
1. criterion = [gini, entropy]
2. max depth = [3,5,10,20,30]
3. max features = [3,5,7,9,11]

[Task-2]
1. Report the Metrics (Accuracy, F1(micro and macro) , Confusion Matrix,
Precision , Recall) for all possible triplet of hyperparamters for both Powerset and MultiOutput Setting.
2. For both of them also rank the top 3 performing set of hyperparamters
according to F1 Score.
3. For the best performing model for each approach report the K Fold validation metrics for an appropriate choice of K While you are allowed to use
sklearn metrics here , we strongly encourage you to write your own code
to compute the metrics and if you do use the sklearn metrics, we strongly
encourage you to verify that it is giving the correct outputs
